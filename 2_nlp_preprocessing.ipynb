{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/enssib_class/blob/main/2_nlp_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z74vm47glyC"
      },
      "source": [
        "#**NLP : tokenizing, lemmatizing, postagging**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt0eU-qsLb10"
      },
      "source": [
        "## What's NLP and why do we need it ?\n",
        "Generally speaking, loads of people use NLP as a preprocessing phase, for further textual treatment. And it is absolutely necessary if you want to avoid noise in statistical analysis and machine learning uses. Basically, preprocessing is **tokenizing**, **lemmatizing** and **postagging**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLIKh_80WtdM"
      },
      "source": [
        "## How do you do that without programming ?\n",
        "I'll show you some basic tools you can easily use without knowing programming in python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31i7LBEP1HF9"
      },
      "source": [
        "##**Some useful online tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P_eXTzr1Uup"
      },
      "source": [
        "###**UDPipe**\n",
        "You'll find it [here](https://lindat.mff.cuni.cz/services/udpipe/).\n",
        "<br>You can use that for short texts.\n",
        "<br>One of its peculiarities is SVG dependency tree building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDSIHDth4kxu"
      },
      "source": [
        "###**Deucalion**\n",
        "You'll find it [here](https://dh.chartes.psl.eu/deucalion/).\n",
        "<br>Much more accurate for longer texts, not an easy output (although ready-to-use format). It's really good on Ancient French and Latin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbFh1tm45B9B"
      },
      "source": [
        "###**VoyantTools**\n",
        "You'll find it [here](https://voyant-tools.org/).\n",
        "<br>It's a visualization, directly online, but for more modules you can build it locally and it's really powerful and neat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj7F-HDpcCkK"
      },
      "source": [
        "#**LE TAL : TOKENISATION, LEMMATISATION, POSTAGGING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQIzGqJzcUVk"
      },
      "source": [
        "We're going to test **`stanza`**. There are loads of other modules on the matter (like`spacy` and `pie-extended`), but `stanza` is fairly easy to use and performs well on loads of languages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "MHhsLVo_LxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try on a simple string first, which is encapsulated in the `catilinaires` variable."
      ],
      "metadata": {
        "id": "Hn8AvFjyNcgH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR5RRUfQc5u4"
      },
      "outputs": [],
      "source": [
        "catilinaires=\"Quousque tandem abutere, Catilina, patientia nostra ? Quamdiu etiam furor iste tuus nos eludet ? Quem ad finem sese effrenata jactabit audacia ? Nihilne te nocturnum praesidium Palatii, nihil urbis vigiliae, nihil timor populi, nihil concursus bonorum omnium, nihil hic munitissimus habendi senatus locus, nihil horum ora vultusque moverunt ? Patere tua consilia non sentis ? Constrictam jam horum omnium scientia teneri conjurationem tuam non vides ? Quid proxima, quid superiore nocte egeris, ubi fueris, quos convocaveris, quid consilii ceperis, quem nostrum ignorare arbitraris ? O tempora ! O mores ! Senatus haec intellegit, consul videt. Hic tamen vivit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_DV1BTw4Cr"
      },
      "source": [
        "##**stanza (précédemment Stanford CoreNLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPEkXPIxeUob"
      },
      "source": [
        "`stanza` has several language models at your disposal (here's a [list](https://stanfordnlp.github.io/stanza/performance.html)), which you can get using the basic language code, like `grc` for Ancient Greek or `la` for Latin. But you can also specify which model you want like below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXi9pljw2DR"
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "stanza.download('la', package=\"perseus\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKCT-NaPxpbp"
      },
      "source": [
        "We begin with building a Pipeline, to indicate which processors we want to use (no need to add `ner` if you don't need named entity recognition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x5kKE7rxTgB"
      },
      "outputs": [],
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='la', package=\"perseus\", processors='tokenize,pos,lemma, depparse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUd62DkjgcvW"
      },
      "source": [
        "Now you can launch the nlp process. Here the `catilinaires_analyzed` is a stanza object, where all the informations you asked for in the Pipeline are encapsulated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPY243ZyE8S"
      },
      "outputs": [],
      "source": [
        "catilinaires_analyzed=nlp_stanza(catilinaires)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(catilinaires_analyzed)"
      ],
      "metadata": {
        "id": "YxQImmS0okFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSS-MkX_gr7m"
      },
      "source": [
        "Here are some results, first for sentence division, and then for lemmatizing and postagging."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`stanza` usually divides the text in sentences, and each sentence is a list of tokens, which each contains attributes such as `.lemma`, or `.pos`."
      ],
      "metadata": {
        "id": "U-ntk1krN7ZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPdO0otBqA8"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  print(\"XXXXX \"+sent.text+\" XXXXX\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNXzUVFZzojD"
      },
      "outputs": [],
      "source": [
        "for sent in catilinaires_analyzed.sentences:\n",
        "  for token in sent.words:\n",
        "    print(token.text + ' - ' + token.lemma + ' - ' + token.pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try with a (much bigger) text."
      ],
      "metadata": {
        "id": "etySMAjIWQZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the text we're going to use is very long, we will use batch processing, that is, launching several processes at once for GPU management. You don't need to understand everything here, but basically we are going to launch several processes at once using the colab GPU."
      ],
      "metadata": {
        "id": "au9q53YfJiCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result will be the same as the one you'll get with a normal process, that is to say, in this case, a list (`words`), which contains each token that has been analyzed by `stanza`."
      ],
      "metadata": {
        "id": "eioMqf8mOcT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, the following cell is just function declaration : it won't do anything unless you call it in your code later."
      ],
      "metadata": {
        "id": "lS9_buiIO6T9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHGyvw9k7HD3"
      },
      "outputs": [],
      "source": [
        "def batch_process(text, nlp, batch_size=50):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            for word in sentence.words:\n",
        "                token={}\n",
        "                if word.lemma is not None:\n",
        "                    token[\"word\"]=word.text\n",
        "                    token[\"lemma\"]=word.lemma\n",
        "                    token[\"pos\"]=word.pos\n",
        "                    words.append(token)\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fr')\n",
        "import string"
      ],
      "metadata": {
        "id": "f2bHYAFOYfpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the _Misérables_."
      ],
      "metadata": {
        "id": "D-eWZAZJKWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/miserables.txt"
      ],
      "metadata": {
        "id": "zgUE5AS2NVSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/miserables.txt\""
      ],
      "metadata": {
        "id": "zCCJtvF2ZP1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Jb1ll9BDZS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma')"
      ],
      "metadata": {
        "id": "bAPzFT46YugW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part may take some time (for me it took something like 4 minutes)."
      ],
      "metadata": {
        "id": "ZGmbsLUeMxac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miserables_analyzed = batch_process(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "xIBpz_4nYjkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(miserables_analyzed[15:25])"
      ],
      "metadata": {
        "id": "x0tlOmanZaAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your own projects, you can get loads of stopword lists [here](https://github.com/stopwords-iso).\n"
      ],
      "metadata": {
        "id": "QRR7NZ8naKEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The upside of this approach (downloading a file) is that you just have to add potential useless words directly into the file you can see by clicking on the folder icon (📁) on the left side of the screen."
      ],
      "metadata": {
        "id": "0R3a6OvkLXF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you accidentally click (after having clicked on the folder icon) on the second folder icon (the open one followed by \"..\"), not to worry : the initial folder is in fact the one that is called \"📁 `content`\", which you can also open."
      ],
      "metadata": {
        "id": "vpoglszWMeB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/stopwords_fr.txt"
      ],
      "metadata": {
        "id": "93Do-k0Ja05o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords_fr.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "-M_2igj5a2KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell, we're going to get every stanza token, and distribute its values in three lists, `forms`, `lemmas` and `no_stop`. In the `forms` list, all the original words are going to be stored (that is to say, basically, the text itself). In the `lemmas` list, the lemmas of the entire text will be stored. Finally, in the `no_stop` list, we will store all the lemmas, except for stopwords and punctuation."
      ],
      "metadata": {
        "id": "xZjdWz6LPIj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then use these lists to show why NLP and pre-processing is necessary to get readable information."
      ],
      "metadata": {
        "id": "plqd3RCpP-P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forms = []\n",
        "lemmas = []\n",
        "no_stop = []\n",
        "\n",
        "for token in miserables_analyzed:\n",
        "    form = token[\"word\"]\n",
        "    lemma = token[\"lemma\"]\n",
        "\n",
        "    if lemma not in string.punctuation:\n",
        "        forms.append(form)\n",
        "        lemmas.append(lemma)\n",
        "\n",
        "    if lemma not in string.punctuation and lemma not in stopwords:\n",
        "        no_stop.append(lemma)"
      ],
      "metadata": {
        "id": "KQMQx7NTa_Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two following cells will give you the number of words stored in the lists. It should be more than 0."
      ],
      "metadata": {
        "id": "BsYWv9SQQKJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(lemmas)"
      ],
      "metadata": {
        "id": "nRAm6ZDsdHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(no_stop)"
      ],
      "metadata": {
        "id": "qGTu3w4hdI4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we're going to build some basic representations of our text to see why preprocessing is important."
      ],
      "metadata": {
        "id": "7yHqu69sLbq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don't need to understand the whole code in the following cell. Just keep this in mind : it creates a word cloud - a visual representation where words from a list are displayed in different sizes based on how often they appear. More frequent words appear larger. In the end, it creates a circular visualization where the most important/frequent words stand out visually through their size, making it easy to quickly identify key themes."
      ],
      "metadata": {
        "id": "UOtonteIQVfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def create_word_cloud(words_list, title):\n",
        "    text = ' '.join(words_list)\n",
        "\n",
        "    radius = 495\n",
        "\n",
        "    diameter = radius * 2\n",
        "    center = radius\n",
        "    x, y = np.ogrid[:diameter, :diameter]\n",
        "    mask = (x - center) ** 2 + (y - center) ** 2 > radius ** 2\n",
        "    mask = 255 * mask.astype(int)\n",
        "\n",
        "    mask_rgba = np.dstack((mask, mask, mask, 255 - mask))\n",
        "\n",
        "    wordcloud = WordCloud(repeat=False, width=diameter, height=diameter,\n",
        "                          background_color=None, mode=\"RGBA\", colormap='plasma',\n",
        "                          mask=mask_rgba).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9qv1DH7LdMhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, a function declaration doesn't do anything on its own : you need to call the function for it to work. The three following cells will create three clouds, one with the `forms` list, one with the `lemmas` list, and one with the `no_stop` list."
      ],
      "metadata": {
        "id": "5zF_WnNLRoYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(forms, 'Word Cloud for Forms')"
      ],
      "metadata": {
        "id": "h-wt9rK1eJpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(lemmas, 'Word Cloud for Lemmas')"
      ],
      "metadata": {
        "id": "t5B5uCAEeL5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_word_cloud(no_stop, 'Word Cloud for Lemmas without stopwords')"
      ],
      "metadata": {
        "id": "pKzLFQ6hec7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If all went well, you'll be able to see that preprocessing is crucial to (visual, but also general) interpretation : the `forms` cloud is basically useless, while the `no_stop` cloud gives you much more information on your text."
      ],
      "metadata": {
        "id": "Gn5l_hECR9vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/forms.txt\", \"w\", encoding=\"utf8\") as f, open(\"/content/lemmas.txt\", \"w\", encoding=\"utf8\") as f2, open(\"/content/pullito.txt\", \"w\", encoding=\"utf8\") as f3:\n",
        "    f.write(\"\\n\".join(forms))\n",
        "    f2.write(\"\\n\".join(lemmas))\n",
        "    f3.write(\"\\n\".join(no_stop))"
      ],
      "metadata": {
        "id": "fXuX7UI9Sgaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DCc3mKreizB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1hxMNZZBdsT9AyRKnXcdOnis82XnHDehl",
      "authorship_tag": "ABX9TyN7ZGpHbkiGHvV2KFYOdrgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}