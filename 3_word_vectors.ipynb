{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/enssib_class/blob/main/3_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUKJJyV4EKwh"
      },
      "source": [
        "# <center>**Word Vectors**</center>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Definition**"
      ],
      "metadata": {
        "id": "lrfpDSkzzK6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try and imagine language as a cloud, with scattered points, where each point is a different word. The location of each point is dependent on the location of every other point in the cloud (eg. if two words share the same context, they should appear near one to another). As long as you can represent a point in space, it gets a computational representation : it becomes a vector in space, a direction. And it becomes possible to compute things from it."
      ],
      "metadata": {
        "id": "RPF5Vq2VzQUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(\"ORstNrlG_2g\", width=512, height=288)"
      ],
      "metadata": {
        "id": "fMH5QvWSzXjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1FsTcOQ5LVgbDqkT5nm_gve5gZfQrZ8pV)"
      ],
      "metadata": {
        "id": "kkeuuYTFze23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the few modules you're going to need. Basically, here's what they do.\n",
        "<br>`os` and `glob` are useful for navigating in your content.\n",
        "\n",
        "*   `os` and `glob` are useful for navigating in your content.\n",
        "*   `gensim` is a module that contains loads of practical tools for basic word vectorization.\n",
        "*   `nltk` is useful here for string manipulation (split in sentences and so on).\n",
        "*   `lxml` is a module for interpreting xml files.\n",
        "*   `string` is a module for basic manipulation.\n",
        "*   `numpy` and `pandas` are generally used for table and matrix manipulations and representations.\n",
        "*   `matplotlib` is a representation/visualization tool."
      ],
      "metadata": {
        "id": "RkyvOkbXsfrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "6YDFgXvZLiiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btSJGNjQEKwj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import glob\n",
        "import nltk\n",
        "\n",
        "from lxml import etree as ET\n",
        "import lxml.html\n",
        "import string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we're going to download a default folder for vector analysis. These texts are in Frantext format. Later on in the notebook, you'll be able to do the same thing with `.txt` files."
      ],
      "metadata": {
        "id": "nsuqGVPduHu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/auteurs.zip"
      ],
      "metadata": {
        "id": "s8-gi9VwSMEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/auteurs.zip\""
      ],
      "metadata": {
        "id": "k3JLxuWlS_jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the authors proposed by default."
      ],
      "metadata": {
        "id": "RDklwrz0urPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt2CYBKmEKwl"
      },
      "outputs": [],
      "source": [
        "flaubert=\"auteurs/flaubert/\"\n",
        "balzac=\"auteurs/balzac/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is basic (but be careful, it's dirty) code for clearing xml code."
      ],
      "metadata": {
        "id": "Mw5s-Ic3uv1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG-aW6vOEKwp"
      },
      "outputs": [],
      "source": [
        "def strip_ns_prefix(tree):\n",
        "    query = \"descendant-or-self::*[namespace-uri()!='']\"\n",
        "    for element in tree.xpath(query):\n",
        "        element.tag = ET.QName(element).localname\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to get each lemma from each word, store it in their sentence, and store each sentence in a list."
      ],
      "metadata": {
        "id": "FjYCOZ8hu-Ht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4tLGaUjEKwp"
      },
      "outputs": [],
      "source": [
        "if balzac != \"\":\n",
        "    files = glob.iglob(balzac + '/**/*.xml', recursive=True)\n",
        "    sentences = []\n",
        "\n",
        "    for filename in files:\n",
        "        print(filename)\n",
        "        parser = ET.XMLParser(remove_blank_text=True, resolve_entities=False, encoding='utf8')\n",
        "        tree = strip_ns_prefix(ET.parse(filename, parser))\n",
        "\n",
        "        words = tree.xpath(\".//wf/@lemma\")\n",
        "\n",
        "        sentence = []\n",
        "        for word in words:\n",
        "            if word != \".\":\n",
        "                sentence.append(word)\n",
        "            else:\n",
        "                sentences.append(sentence + [word])\n",
        "                sentence = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to check wether it worked (something should be printed)."
      ],
      "metadata": {
        "id": "uKkzOqYjvQJD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWKGOPhgEKwq"
      },
      "outputs": [],
      "source": [
        "print(len(sentences))\n",
        "print(sentences[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Xl4zaIEKwr"
      },
      "source": [
        "## Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part, depending on the amount of data you intend to compute, may take some time (default : 8 minutes)"
      ],
      "metadata": {
        "id": "XLU6gj_Afxmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RatPiNJ2EKws"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=10, epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4yuImONEKws"
      },
      "outputs": [],
      "source": [
        "model.wv.save(\"/content/model_balzac.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next cell is to be run only if you want to reload a saved model."
      ],
      "metadata": {
        "id": "nCOsjlH0QC48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "KeyedVectors.load(\"/content/model_balzac.bin\")\n",
        "wv = KeyedVectors.load(\"/content/model_balzac.bin\")\n",
        "\n",
        "model = Word2Vec(vector_size=wv.vector_size, min_count=1)\n",
        "model.wv = wv"
      ],
      "metadata": {
        "id": "QAktcdnKNPpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell shows analogies between vectors : tell me, if I give you the link between \"queen\" and \"king\", the equivalent for \"man\", and you should get something like \"woman\" or \"girl\", depending on your corpus."
      ],
      "metadata": {
        "id": "20SaPPldvd8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czf2ZjP9EKws"
      },
      "outputs": [],
      "source": [
        "#Paris is to France what London is to what ? model.wv.most_similar(positive=['Londres', 'France'], negative=['Paris'],topn=5)\n",
        "#King is to man what Queen is to what ? model.wv.most_similar(positive=['reine', 'homme'], negative=['roi'],topn=5)\n",
        "model.wv.most_similar(positive=['reine', 'homme'], negative=['roi'],topn=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the default value `'esprit'` here."
      ],
      "metadata": {
        "id": "PQQiKaTIv-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('esprit',topn=20)"
      ],
      "metadata": {
        "id": "M3poyzGldb4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization with Tensorflow\n",
        "You can get a clearer visualization using the [online tensorflow visualizer](https://projector.tensorflow.org/). After this next cell, you'll get two files, one containing the vectors, the other their labels."
      ],
      "metadata": {
        "id": "dS_L_ZjGvDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/OdysseusPolymetis/enexdi2025_prep/refs/heads/main/stopwords_fr.txt"
      ],
      "metadata": {
        "id": "ePwXdpLrU3JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stops = open(\"/content/stopwords_fr.txt\", encoding=\"utf-8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "hcgp0bLSU47L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/vecteurs.tsv\", 'w') as file_vectors, open(\"/content/metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "IRRnV7TbVlI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same thing with your own TXT"
      ],
      "metadata": {
        "id": "G68iG-oGwIpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we don't have pre-processed text, we need to preprocess it a bit. We'll use `stanza` for lemmatization."
      ],
      "metadata": {
        "id": "zxBfdJF-xWwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "0zSKqbFWwNi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also use a list of stopwords from a basic repo (several languages) : you can get other lists from [here](https://github.com/stopwords-iso). You'll have to look for your language, and get the `.txt` file in the adequate folder. When you see it, visualize it in \"raw\", and copy the url (and paste it in the next cell)."
      ],
      "metadata": {
        "id": "06ABwLoGxjgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/refs/heads/master/stopwords-fr.txt"
      ],
      "metadata": {
        "id": "KIEGfTEgwZ5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = open(\"/content/stopwords-fr.txt\",'r',encoding=\"utf8\").read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "C37Oxfujwh_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can upload your own text by clicking on the folder icon to your left (üìÅ), and drop your `.txt` there.\n",
        "<br>Next you can modify the following cell by changing the title. Careful though, also as a general rule, no spaces, no accents, no special characters in the title."
      ],
      "metadata": {
        "id": "dM48GwBDyoiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_of_text = \"/content/3mousquetaires.txt\""
      ],
      "metadata": {
        "id": "Ra7Hpj7dwmWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
      ],
      "metadata": {
        "id": "Rbz_Jdx7wqZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the model used is French. You can choose your own model from this [list](https://stanfordnlp.github.io/stanza/performance.html). Just change `fr` in the two following cells."
      ],
      "metadata": {
        "id": "FSwMb0ulzu10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('fr')"
      ],
      "metadata": {
        "id": "FQy7AuFUwuIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_stanza = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma')"
      ],
      "metadata": {
        "id": "JZsPQQ8Kwwfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part is just a way to use the GPU better."
      ],
      "metadata": {
        "id": "IlY1UNAA0CR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_process_to_lemmas(text, nlp, batch_size=100):\n",
        "    paragraphs = text.split('\\n')\n",
        "    batches = [paragraphs[i:i + batch_size] for i in range(0, len(paragraphs), batch_size)]\n",
        "\n",
        "    sentences_lemmas = []\n",
        "\n",
        "    for batch in batches:\n",
        "        batch_text = '\\n'.join(batch)\n",
        "        doc = nlp(batch_text)\n",
        "        for sentence in doc.sentences:\n",
        "            sentence_lemmas = []\n",
        "            for word in sentence.words:\n",
        "                if word.lemma is not None and word.lemma not in stopwords:\n",
        "                    sentence_lemmas.append(word.lemma.lower())\n",
        "            sentences_lemmas.append(sentence_lemmas)\n",
        "\n",
        "    return sentences_lemmas"
      ],
      "metadata": {
        "id": "NyCS-Orzw5KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = batch_process_to_lemmas(full_text, nlp_stanza)"
      ],
      "metadata": {
        "id": "RJmTG6pcw8b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences, min_count=2, max_vocab_size=10000, negative=10, epochs=300)"
      ],
      "metadata": {
        "id": "P3O6A0qSxAxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.save(\"/content/yourModel.bin\")"
      ],
      "metadata": {
        "id": "oalaIYUFxHO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('courage',topn=50)"
      ],
      "metadata": {
        "id": "RtK7iebfxNG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/vecteurs.tsv\", 'w') as file_vectors, open(\"/content/metadonnees.tsv\", 'w') as file_metadata:\n",
        "    for word in model.wv.index_to_key:\n",
        "        file_vectors.write('\\t'.join([str(x) for x in model.wv[word]]) + \"\\n\")\n",
        "        file_metadata.write(word + \"\\n\")"
      ],
      "metadata": {
        "id": "M2Rk0YSzcBx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}